{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc51607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a83d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datasets import Dataset, list_metrics, load_metric\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from argument_parsing.model_args import ModelArguments\n",
    "from argument_parsing.data_args import DataArguments\n",
    "from argument_parsing.experiment_args import ExperimentArguments\n",
    "from runners.run_mlm_exp import ExpRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f9fcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "gpuids = [0, 1, 2, 3]\n",
    "\n",
    "if gpuids is None or len(gpuids) == 0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    print(\"Using CPU\")\n",
    "else:\n",
    "    gpuid_str = str(gpuids[0])\n",
    "    for gpuid in gpuids[1:]:\n",
    "        gpuid_str += \",{}\".format(gpuid)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid_str\n",
    "    print(\"Using GPU:{}\".format(gpuid_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3240d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef15c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ExperimentArguments, ModelArguments, DataArguments, TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8d93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args, model_args, data_args, training_args = parser.parse_json_file(json_file=\"../src/argument_configs/temp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2ad61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-06-15 11:24:03,750 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /data/ddmg/personalizedmentalhealth/reddit_project/cached/temp/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:553] 2021-06-15 11:24:03,753 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset to load.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1717] 2021-06-15 11:24:04,086 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /data/ddmg/personalizedmentalhealth/reddit_project/cached/temp/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-15 11:24:04,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /data/ddmg/personalizedmentalhealth/reddit_project/cached/temp/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-15 11:24:04,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-15 11:24:04,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-15 11:24:04,090 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /data/ddmg/personalizedmentalhealth/reddit_project/cached/temp/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:515] 2021-06-15 11:24:04,138 >> loading configuration file /data/ddmg/personalizedmentalhealth/reddit_project/results/2_bert_finetuned/checkpoint-19500/config.json\n",
      "[INFO|configuration_utils.py:553] 2021-06-15 11:24:04,139 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1153] 2021-06-15 11:24:04,141 >> loading weights file /data/ddmg/personalizedmentalhealth/reddit_project/results/2_bert_finetuned/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1339] 2021-06-15 11:24:05,679 >> All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-15 11:24:05,681 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /data/ddmg/personalizedmentalhealth/reddit_project/results/2_bert_finetuned/checkpoint-19500/pytorch_model.bin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded existing pred data\n"
     ]
    }
   ],
   "source": [
    "exp_runner = ExpRunner(exp_args, model_args, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b5bab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:515] 2021-06-15 11:21:54,547 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, created_utc, text, data_split, id, author, __index_level_0__, subreddit.\n",
      "[INFO|trainer.py:2115] 2021-06-15 11:21:54,554 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-15 11:21:54,555 >>   Num examples = 16\n",
      "[INFO|trainer.py:2120] 2021-06-15 11:21:54,556 >>   Batch size = 32\n",
      "/data/ddmg/users/kmatton/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics1 = exp_runner.trainer.evaluate(exp_runner.pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfac3e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.623096227645874,\n",
       " 'eval_runtime': 6.8626,\n",
       " 'eval_samples_per_second': 2.331}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c08bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:515] 2021-06-15 11:22:03,318 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, created_utc, text, data_split, id, author, __index_level_0__, subreddit.\n",
      "[INFO|trainer.py:2115] 2021-06-15 11:22:03,326 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2117] 2021-06-15 11:22:03,327 >>   Num examples = 16\n",
      "[INFO|trainer.py:2120] 2021-06-15 11:22:03,328 >>   Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "metrics2 = exp_runner.trainer.evaluate(exp_runner.pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b64e89e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6316421031951904,\n",
       " 'eval_runtime': 0.2832,\n",
       " 'eval_samples_per_second': 56.496}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeaa633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runner.trainer.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d355607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:515] 2021-06-15 11:24:17,979 >> The following columns in the test set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: data_split, created_utc, __index_level_0__, id, subreddit, author, special_tokens_mask, text.\n"
     ]
    }
   ],
   "source": [
    "dl = exp_runner.trainer.get_test_dataloader(exp_runner.pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a0dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6298, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "exp_runner.trainer.model.eval()\n",
    "for step, inputs in enumerate(dl):\n",
    "    loss, logits, labels = exp_runner.trainer.prediction_step(\n",
    "        exp_runner.trainer.model,\n",
    "        inputs,\n",
    "        False,\n",
    "    )\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d69230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2339dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_pt_utils import nested_detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae03e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'input_ids': tensor([[  101,  2026,  3566,  ...,  3262,  2138,   102],\n",
      "        [  101,  2129,  2064,  ...,     0,     0,     0],\n",
      "        [  101, 27393,  9504,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   103,  3314,  ...,     0,     0,     0],\n",
      "        [  101,   103,  2005,  ...,     0,     0,     0],\n",
      "        [  101,  2026,  3566,  ...,   103,  3047,   102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  ...,  3262,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2026,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ -100, 20565,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100, 11922,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2008,  -100,  -100]], device='cuda:0')}\n",
      "True\n",
      "tensor([[ -100,  -100,  -100,  ...,  3262,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  2026,  -100,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ -100, 20565,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100, 11922,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  ...,  2008,  -100,  -100]], device='cuda:0')\n",
      "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'input_ids': tensor([[  101,  2026,  3566,  ...,   103,  2138,   102],\n",
      "        [  101,  2129,  2064,  ...,     0,     0,     0],\n",
      "        [  101,  2026,  9504,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 20565,  3314,  ...,     0,     0,     0],\n",
      "        [  101, 11922,  2005,  ...,     0,     0,     0],\n",
      "        [  101,  2026, 21211,  ...,  2008,  3047,   102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[-100, -100, -100,  ..., 3262, 2138, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2005,  ..., -100, -100, -100],\n",
      "        [-100, -100, 3566,  ..., -100, -100, -100]], device='cuda:0')}\n",
      "True\n",
      "tensor([[-100, -100, -100,  ..., 3262, 2138, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, 2005,  ..., -100, -100, -100],\n",
      "        [-100, -100, 3566,  ..., -100, -100, -100]], device='cuda:0')\n",
      "tensor(1.4743, device='cuda:0')\n",
      "tensor(1.4819, device='cuda:0')\n",
      "tensor(1.4819, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "exp_runner.trainer.model.eval()\n",
    "for step, inputs in enumerate(dl):\n",
    "    inputs1 = inputs\n",
    "    has_labels = all(inputs.get(k) is not None for k in exp_runner.trainer.label_names)\n",
    "    inputs = exp_runner.trainer._prepare_inputs(inputs)\n",
    "    if hasattr(exp_runner.trainer.model, \"config\"):\n",
    "        ignore_keys = getattr(exp_runner.trainer.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "    else:\n",
    "        ignore_keys = []\n",
    "\n",
    "    # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "    if has_labels:\n",
    "        labels = nested_detach(tuple(inputs.get(name) for name in exp_runner.trainer.label_names))\n",
    "        if len(labels) == 1:\n",
    "            labels = labels[0]\n",
    "    else:\n",
    "        labels = None\n",
    "    print(inputs)\n",
    "    print(has_labels)\n",
    "    print(labels)\n",
    "    with torch.no_grad():\n",
    "        loss1, outputs = exp_runner.trainer.compute_loss(\n",
    "            exp_runner.trainer.model,\n",
    "            inputs,\n",
    "            return_outputs=True\n",
    "        )\n",
    "        \n",
    "exp_runner.trainer.model.eval()\n",
    "for step, inputs in enumerate(dl):\n",
    "    inputs2 = inputs\n",
    "    has_labels = all(inputs.get(k) is not None for k in exp_runner.trainer.label_names)\n",
    "    inputs = exp_runner.trainer._prepare_inputs(inputs)\n",
    "    if hasattr(exp_runner.trainer.model, \"config\"):\n",
    "        ignore_keys = getattr(exp_runner.trainer.model.config, \"keys_to_ignore_at_inference\", [])\n",
    "    else:\n",
    "        ignore_keys = []\n",
    "\n",
    "    # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n",
    "    if has_labels:\n",
    "        labels = nested_detach(tuple(inputs.get(name) for name in exp_runner.trainer.label_names))\n",
    "        if len(labels) == 1:\n",
    "            labels = labels[0]\n",
    "    else:\n",
    "        labels = None\n",
    "    print(inputs)\n",
    "    print(has_labels)\n",
    "    print(labels)\n",
    "    with torch.no_grad():\n",
    "        loss2, outputs = exp_runner.trainer.compute_loss(\n",
    "            exp_runner.trainer.model,\n",
    "            inputs,\n",
    "            return_outputs=True\n",
    "        )\n",
    "        loss3, outputs = exp_runner.trainer.compute_loss(\n",
    "            exp_runner.trainer.model,\n",
    "            inputs,\n",
    "            return_outputs=True\n",
    "        )\n",
    "    print(loss1)\n",
    "    print(loss2)\n",
    "    print(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a5a9e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 11.91 GiB total capacity; 10.46 GiB already allocated; 700.94 MiB free; 10.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e4d007608ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     loss, outputs = exp_runner.trainer.compute_loss(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mexp_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -100 index = padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 11.91 GiB total capacity; 10.46 GiB already allocated; 700.94 MiB free; 10.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "exp_runner.trainer.model.eval()\n",
    "for step, inputs in enumerate(dl):\n",
    "    inputs = exp_runner.trainer._prepare_inputs(inputs)\n",
    "    with torch.no_grad():\n",
    "        loss, outputs = exp_runner.trainer.compute_loss(\n",
    "            exp_runner.trainer.model,\n",
    "            inputs,\n",
    "            return_outputs=True\n",
    "        )\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525deaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runner.trainer.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c993c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
