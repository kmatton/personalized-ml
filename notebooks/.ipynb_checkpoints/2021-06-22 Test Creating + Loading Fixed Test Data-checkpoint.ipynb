{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1deacd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1aacc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datasets import Dataset, list_metrics, load_metric, load_from_disk\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, HfArgumentParser, TrainingArguments, default_data_collator\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from argument_parsing.model_args import ModelArguments\n",
    "from argument_parsing.data_args import DataArguments\n",
    "from argument_parsing.experiment_args import ExperimentArguments\n",
    "from runners.run_mlm_exp import ExpRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cdbea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "gpuids = [0, 1, 2, 3]\n",
    "\n",
    "if gpuids is None or len(gpuids) == 0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    print(\"Using CPU\")\n",
    "else:\n",
    "    gpuid_str = str(gpuids[0])\n",
    "    for gpuid in gpuids[1:]:\n",
    "        gpuid_str += \",{}\".format(gpuid)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid_str\n",
    "    print(\"Using GPU:{}\".format(gpuid_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8e1db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f723b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ExperimentArguments, ModelArguments, DataArguments, TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6434e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_args, model_args, data_args, training_args = parser.parse_json_file(json_file=\"../src/argument_configs/temp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85aaab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-06-22 09:39:17,930 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /data/ddmg/redditlanguagemodeling/cached/temp/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
      "[INFO|configuration_utils.py:553] 2021-06-22 09:39:17,932 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0,1,2,3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1717] 2021-06-22 09:39:18,282 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /data/ddmg/redditlanguagemodeling/cached/temp/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-22 09:39:18,285 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /data/ddmg/redditlanguagemodeling/cached/temp/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-22 09:39:18,286 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-22 09:39:18,287 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-06-22 09:39:18,288 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /data/ddmg/redditlanguagemodeling/cached/temp/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:517] 2021-06-22 09:39:18,399 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /data/ddmg/redditlanguagemodeling/cached/temp/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
      "[INFO|configuration_utils.py:553] 2021-06-22 09:39:18,401 >> Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1155] 2021-06-22 09:39:18,482 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /data/ddmg/redditlanguagemodeling/cached/temp/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "[INFO|modeling_utils.py:1339] 2021-06-22 09:39:19,320 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-06-22 09:39:19,321 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load pre-existing training dataset. Reloading...\n",
      "Did not find existing datasets. Reloading..\n"
     ]
    }
   ],
   "source": [
    "exp_runner = ExpRunner(exp_args, model_args, data_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3560995f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'author', 'data_split', 'subreddit', 'text', 'created_utc', '__index_level_0__'],\n",
       "    num_rows: 521\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runner.pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30da6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = exp_runner.pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c5cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_runner.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f236c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7efc78f61040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ff77d072da4e1b9304292cba4d1556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_data = pred_data.map(\n",
    "    lambda x: exp_runner.tokenizer(x[\"text\"], padding='max_length', truncation=True, return_special_tokens_mask=True),\n",
    "    batched=True,\n",
    "    num_proc=exp_runner.data_args.preprocessing_num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d67110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{512,\n",
       " 519,\n",
       " 525,\n",
       " 526,\n",
       " 531,\n",
       " 538,\n",
       " 540,\n",
       " 547,\n",
       " 548,\n",
       " 556,\n",
       " 560,\n",
       " 569,\n",
       " 571,\n",
       " 577,\n",
       " 589,\n",
       " 592,\n",
       " 626,\n",
       " 631,\n",
       " 655,\n",
       " 659,\n",
       " 678,\n",
       " 690,\n",
       " 702,\n",
       " 710,\n",
       " 721,\n",
       " 722,\n",
       " 774,\n",
       " 818,\n",
       " 853,\n",
       " 861,\n",
       " 891,\n",
       " 901,\n",
       " 905,\n",
       " 909,\n",
       " 973,\n",
       " 1053,\n",
       " 1202,\n",
       " 1308,\n",
       " 1372,\n",
       " 1387,\n",
       " 1405,\n",
       " 1406,\n",
       " 2492,\n",
       " 2505}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([len(x) for x in pred_data['input_ids']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
