{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd3f20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from dataset_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d874b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"/data/ddmg/personalizedmentalhealth/reddit_project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045f1d0",
   "metadata": {},
   "source": [
    "# Load Data and Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67cccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [os.path.join(BASE_DATA_PATH, \"4_all_data.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9477e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for data_path in data_paths:\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e659bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>data_split</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2rrm7n</td>\n",
       "      <td>Fearlessfight</td>\n",
       "      <td>train</td>\n",
       "      <td>family</td>\n",
       "      <td>My Mum is acting really weird. This is weird b...</td>\n",
       "      <td>1.420742e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2rgc78</td>\n",
       "      <td>AstroKate</td>\n",
       "      <td>train</td>\n",
       "      <td>family</td>\n",
       "      <td>Awkward Situations w/ My Dad? Hi Everyone! I'm...</td>\n",
       "      <td>1.420501e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2rdr14</td>\n",
       "      <td>bigguytx</td>\n",
       "      <td>train</td>\n",
       "      <td>family</td>\n",
       "      <td>Just found out my father isn't my father (afte...</td>\n",
       "      <td>1.420442e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2rbrfq</td>\n",
       "      <td>staceyastacey</td>\n",
       "      <td>train</td>\n",
       "      <td>family</td>\n",
       "      <td>Our mother left her family A public letter to ...</td>\n",
       "      <td>1.420402e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2r6afm</td>\n",
       "      <td>itlikesmenot</td>\n",
       "      <td>train</td>\n",
       "      <td>family</td>\n",
       "      <td>Sister's shit of a \"man\" So my sister has 2 ki...</td>\n",
       "      <td>1.420260e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379454</th>\n",
       "      <td>knek0k</td>\n",
       "      <td>90dayfinancee</td>\n",
       "      <td>train</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>HELP! \"Large\" savings account, pay down debt o...</td>\n",
       "      <td>1.609374e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379455</th>\n",
       "      <td>kneht6</td>\n",
       "      <td>callmeqws</td>\n",
       "      <td>train</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Help me understand my statement balance? [http...</td>\n",
       "      <td>1.609374e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379456</th>\n",
       "      <td>kneh32</td>\n",
       "      <td>DogtorPepper</td>\n",
       "      <td>train</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>HSA Account Fees I have an old HSA from old em...</td>\n",
       "      <td>1.609374e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379457</th>\n",
       "      <td>knedb8</td>\n",
       "      <td>apenguin7</td>\n",
       "      <td>train</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Am I (son) responsible for my deceased father'...</td>\n",
       "      <td>1.609373e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379458</th>\n",
       "      <td>kne60p</td>\n",
       "      <td>IamManfred</td>\n",
       "      <td>train</td>\n",
       "      <td>personalfinance</td>\n",
       "      <td>Can someone explain to me what exemptions are ...</td>\n",
       "      <td>1.609373e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379459 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id         author data_split        subreddit  \\\n",
       "0        2rrm7n  Fearlessfight      train           family   \n",
       "1        2rgc78      AstroKate      train           family   \n",
       "2        2rdr14       bigguytx      train           family   \n",
       "3        2rbrfq  staceyastacey      train           family   \n",
       "4        2r6afm   itlikesmenot      train           family   \n",
       "...         ...            ...        ...              ...   \n",
       "1379454  knek0k  90dayfinancee      train  personalfinance   \n",
       "1379455  kneht6      callmeqws      train  personalfinance   \n",
       "1379456  kneh32   DogtorPepper      train  personalfinance   \n",
       "1379457  knedb8      apenguin7      train  personalfinance   \n",
       "1379458  kne60p     IamManfred      train  personalfinance   \n",
       "\n",
       "                                                      text   created_utc  \n",
       "0        My Mum is acting really weird. This is weird b...  1.420742e+09  \n",
       "1        Awkward Situations w/ My Dad? Hi Everyone! I'm...  1.420501e+09  \n",
       "2        Just found out my father isn't my father (afte...  1.420442e+09  \n",
       "3        Our mother left her family A public letter to ...  1.420402e+09  \n",
       "4        Sister's shit of a \"man\" So my sister has 2 ki...  1.420260e+09  \n",
       "...                                                    ...           ...  \n",
       "1379454  HELP! \"Large\" savings account, pay down debt o...  1.609374e+09  \n",
       "1379455  Help me understand my statement balance? [http...  1.609374e+09  \n",
       "1379456  HSA Account Fees I have an old HSA from old em...  1.609374e+09  \n",
       "1379457  Am I (son) responsible for my deceased father'...  1.609373e+09  \n",
       "1379458  Can someone explain to me what exemptions are ...  1.609373e+09  \n",
       "\n",
       "[1379459 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed4036d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My Mum is acting really weird. This is weird but I wondered if I could have some help. My mum has been acting strange for a while now. She just doesn't smile anymore and has a blank facial expression. She does suffer with insomnia and she has been very and I mean very stressed. We do have to move house soon and we are lacking money but her behaviour is still really peculiar for her. She doesn't listen to things I say and she has been judging my opinions and also laughing when it's not funny and not laughing when it is funny. It's really worrying me, as I'm only 14. But what do I do? It's so horrible\",\n",
       " 'Awkward Situations w/ My Dad? Hi Everyone! I\\'m Jay 13 years old, I have a problem with my dad. I always feel awkward when we\\'ll watch tv, or see magazines, etc. For example if \"Dancing w/ The Stars\" is on the woman will have skimpy clothing on and, I feel my face getting hot, and I get embarrassed. Or what I have a bigger problem with is when we\\'re at walmart and going through the check out line, there\\'s usually \"Maxim\" magazines there which really makes me embarrassed/awkward. Also he never says anything to me about girls or anything, like you know how dad\\'s we\\'ll tease there sons like saying: \"look at that pretty lady\" or something along those lines, he never does that. I do have a Kate Upton poster in my room. He\\'s never gave me \"The Talk\" either. This makes my uncomfortable with asking him if I can have a girlfriend. Around August last year he I had my xbox 360 home background a Kate Upton picture (as you can tell I like her a lot, lol) And he said \"Why you got girls in bathing suits on your xbox boy\" he said it in a joking way, and smiled. So it was nothing negative. And the only other thing he did was, said to me while I was playing xbox was \"Jay, that looks like a fun game\" and I turned around right before it turned off, and I heard the audio, and I\\'m 99% sure it was the \"Kate Upton Game of War\" commercial cause I said: \"oh, I missed it what was it called?\" and he said \"Game of War or something\" do you think he was hinting at it cause he knew it was the same girl I had on poster in my room? HELP!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch = list(df['text'].values[0:2])\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad48cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "321a1ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2026, 12954, 2003, 3772, 2428, 6881, 1012, 2023, 2003, 6881, 2021, 1045, 4999, 2065, 1045, 2071, 2031, 2070, 2393, 1012, 2026, 12954, 2038, 2042, 3772, 4326, 2005, 1037, 2096, 2085, 1012, 2016, 2074, 2987, 1005, 1056, 2868, 4902, 1998, 2038, 1037, 8744, 13268, 3670, 1012, 2016, 2515, 9015, 2007, 16021, 5358, 6200, 1998, 2016, 2038, 2042, 2200, 1998, 1045, 2812, 2200, 13233, 1012, 2057, 2079, 2031, 2000, 2693, 2160, 2574, 1998, 2057, 2024, 11158, 2769, 2021, 2014, 9164, 2003, 2145, 2428, 14099, 2005, 2014, 1012, 2016, 2987, 1005, 1056, 4952, 2000, 2477, 1045, 2360, 1998, 2016, 2038, 2042, 13325, 2026, 10740, 1998, 2036, 5870, 2043, 2009, 1005, 1055, 2025, 6057, 1998, 2025, 5870, 2043, 2009, 2003, 6057, 1012, 2009, 1005, 1055, 2428, 15366, 2033, 1010, 2004, 1045, 1005, 1049, 2069, 2403, 1012, 2021, 2054, 2079, 1045, 2079, 1029, 2009, 1005, 1055, 2061, 9202, 102], [101, 9596, 8146, 1059, 1013, 2026, 3611, 1029, 7632, 3071, 999, 1045, 1005, 1049, 6108, 2410, 2086, 2214, 1010, 1045, 2031, 1037, 3291, 2007, 2026, 3611, 1012, 1045, 2467, 2514, 9596, 2043, 2057, 1005, 2222, 3422, 2694, 1010, 2030, 2156, 7298, 1010, 4385, 1012, 2005, 2742, 2065, 1000, 5613, 1059, 1013, 1996, 3340, 1000, 2003, 2006, 1996, 2450, 2097, 2031, 8301, 8737, 2100, 5929, 2006, 1998, 1010, 1045, 2514, 2026, 2227, 2893, 2980, 1010, 1998, 1045, 2131, 10339, 1012, 2030, 2054, 1045, 2031, 1037, 7046, 3291, 2007, 2003, 2043, 2057, 1005, 2128, 2012, 24547, 22345, 1998, 2183, 2083, 1996, 4638, 2041, 2240, 1010, 2045, 1005, 1055, 2788, 1000, 20446, 1000, 7298, 2045, 2029, 2428, 3084, 2033, 10339, 1013, 9596, 1012, 2036, 2002, 2196, 2758, 2505, 2000, 2033, 2055, 3057, 2030, 2505, 1010, 2066, 2017, 2113, 2129, 3611, 1005, 1055, 2057, 1005, 2222, 18381, 2045, 4124, 2066, 3038, 1024, 1000, 2298, 2012, 2008, 3492, 3203, 1000, 2030, 2242, 2247, 2216, 3210, 1010, 2002, 2196, 2515, 2008, 1012, 1045, 2079, 2031, 1037, 5736, 26900, 13082, 1999, 2026, 2282, 1012, 2002, 1005, 1055, 2196, 2435, 2033, 1000, 1996, 2831, 1000, 2593, 1012, 2023, 3084, 2026, 8796, 2007, 4851, 2032, 2065, 1045, 2064, 2031, 1037, 6513, 1012, 2105, 2257, 2197, 2095, 2002, 1045, 2018, 2026, 12202, 9475, 2188, 4281, 1037, 5736, 26900, 3861, 1006, 2004, 2017, 2064, 2425, 1045, 2066, 2014, 1037, 2843, 1010, 8840, 2140, 1007, 1998, 2002, 2056, 1000, 2339, 2017, 2288, 3057, 1999, 17573, 11072, 2006, 2115, 12202, 2879, 1000, 2002, 2056, 2009, 1999, 1037, 16644, 2126, 1010, 1998, 3281, 1012, 2061, 2009, 2001, 2498, 4997, 1012, 1998, 1996, 2069, 2060, 2518, 2002, 2106, 2001, 1010, 2056, 2000, 2033, 2096, 1045, 2001, 2652, 12202, 2001, 1000, 6108, 1010, 2008, 3504, 2066, 1037, 4569, 2208, 1000, 1998, 1045, 2357, 2105, 2157, 2077, 2009, 2357, 2125, 1010, 1998, 1045, 2657, 1996, 5746, 1010, 1998, 1045, 1005, 1049, 5585, 1003, 2469, 2009, 2001, 1996, 1000, 5736, 26900, 2208, 1997, 2162, 1000, 3293, 3426, 1045, 2056, 1024, 1000, 2821, 1010, 1045, 4771, 2009, 2054, 2001, 2009, 2170, 1029, 1000, 1998, 2002, 2056, 1000, 2208, 1997, 2162, 2030, 2242, 1000, 2079, 2017, 2228, 2002, 2001, 9374, 2075, 2012, 2009, 3426, 2002, 2354, 2009, 2001, 1996, 2168, 2611, 1045, 2018, 2006, 13082, 1999, 2026, 2282, 1029, 2393, 999, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(example_batch)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eda7f937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c05031a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "824723a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'awkward',\n",
       " 'situations',\n",
       " 'w',\n",
       " '/',\n",
       " 'my',\n",
       " 'dad',\n",
       " '?',\n",
       " 'hi',\n",
       " 'everyone',\n",
       " '!',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'jay',\n",
       " '13',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'my',\n",
       " 'dad',\n",
       " '.',\n",
       " 'i',\n",
       " 'always',\n",
       " 'feel',\n",
       " 'awkward',\n",
       " 'when',\n",
       " 'we',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'watch',\n",
       " 'tv',\n",
       " ',',\n",
       " 'or',\n",
       " 'see',\n",
       " 'magazines',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'for',\n",
       " 'example',\n",
       " 'if',\n",
       " '\"',\n",
       " 'dancing',\n",
       " 'w',\n",
       " '/',\n",
       " 'the',\n",
       " 'stars',\n",
       " '\"',\n",
       " 'is',\n",
       " 'on',\n",
       " 'the',\n",
       " 'woman',\n",
       " 'will',\n",
       " 'have',\n",
       " 'ski',\n",
       " '##mp',\n",
       " '##y',\n",
       " 'clothing',\n",
       " 'on',\n",
       " 'and',\n",
       " ',',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'my',\n",
       " 'face',\n",
       " 'getting',\n",
       " 'hot',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'get',\n",
       " 'embarrassed',\n",
       " '.',\n",
       " 'or',\n",
       " 'what',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bigger',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'is',\n",
       " 'when',\n",
       " 'we',\n",
       " \"'\",\n",
       " 're',\n",
       " 'at',\n",
       " 'wal',\n",
       " '##mart',\n",
       " 'and',\n",
       " 'going',\n",
       " 'through',\n",
       " 'the',\n",
       " 'check',\n",
       " 'out',\n",
       " 'line',\n",
       " ',',\n",
       " 'there',\n",
       " \"'\",\n",
       " 's',\n",
       " 'usually',\n",
       " '\"',\n",
       " 'maxim',\n",
       " '\"',\n",
       " 'magazines',\n",
       " 'there',\n",
       " 'which',\n",
       " 'really',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'embarrassed',\n",
       " '/',\n",
       " 'awkward',\n",
       " '.',\n",
       " 'also',\n",
       " 'he',\n",
       " 'never',\n",
       " 'says',\n",
       " 'anything',\n",
       " 'to',\n",
       " 'me',\n",
       " 'about',\n",
       " 'girls',\n",
       " 'or',\n",
       " 'anything',\n",
       " ',',\n",
       " 'like',\n",
       " 'you',\n",
       " 'know',\n",
       " 'how',\n",
       " 'dad',\n",
       " \"'\",\n",
       " 's',\n",
       " 'we',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'tease',\n",
       " 'there',\n",
       " 'sons',\n",
       " 'like',\n",
       " 'saying',\n",
       " ':',\n",
       " '\"',\n",
       " 'look',\n",
       " 'at',\n",
       " 'that',\n",
       " 'pretty',\n",
       " 'lady',\n",
       " '\"',\n",
       " 'or',\n",
       " 'something',\n",
       " 'along',\n",
       " 'those',\n",
       " 'lines',\n",
       " ',',\n",
       " 'he',\n",
       " 'never',\n",
       " 'does',\n",
       " 'that',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " 'have',\n",
       " 'a',\n",
       " 'kate',\n",
       " 'upton',\n",
       " 'poster',\n",
       " 'in',\n",
       " 'my',\n",
       " 'room',\n",
       " '.',\n",
       " 'he',\n",
       " \"'\",\n",
       " 's',\n",
       " 'never',\n",
       " 'gave',\n",
       " 'me',\n",
       " '\"',\n",
       " 'the',\n",
       " 'talk',\n",
       " '\"',\n",
       " 'either',\n",
       " '.',\n",
       " 'this',\n",
       " 'makes',\n",
       " 'my',\n",
       " 'uncomfortable',\n",
       " 'with',\n",
       " 'asking',\n",
       " 'him',\n",
       " 'if',\n",
       " 'i',\n",
       " 'can',\n",
       " 'have',\n",
       " 'a',\n",
       " 'girlfriend',\n",
       " '.',\n",
       " 'around',\n",
       " 'august',\n",
       " 'last',\n",
       " 'year',\n",
       " 'he',\n",
       " 'i',\n",
       " 'had',\n",
       " 'my',\n",
       " 'xbox',\n",
       " '360',\n",
       " 'home',\n",
       " 'background',\n",
       " 'a',\n",
       " 'kate',\n",
       " 'upton',\n",
       " 'picture',\n",
       " '(',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'i',\n",
       " 'like',\n",
       " 'her',\n",
       " 'a',\n",
       " 'lot',\n",
       " ',',\n",
       " 'lo',\n",
       " '##l',\n",
       " ')',\n",
       " 'and',\n",
       " 'he',\n",
       " 'said',\n",
       " '\"',\n",
       " 'why',\n",
       " 'you',\n",
       " 'got',\n",
       " 'girls',\n",
       " 'in',\n",
       " 'bathing',\n",
       " 'suits',\n",
       " 'on',\n",
       " 'your',\n",
       " 'xbox',\n",
       " 'boy',\n",
       " '\"',\n",
       " 'he',\n",
       " 'said',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'joking',\n",
       " 'way',\n",
       " ',',\n",
       " 'and',\n",
       " 'smiled',\n",
       " '.',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'negative',\n",
       " '.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'only',\n",
       " 'other',\n",
       " 'thing',\n",
       " 'he',\n",
       " 'did',\n",
       " 'was',\n",
       " ',',\n",
       " 'said',\n",
       " 'to',\n",
       " 'me',\n",
       " 'while',\n",
       " 'i',\n",
       " 'was',\n",
       " 'playing',\n",
       " 'xbox',\n",
       " 'was',\n",
       " '\"',\n",
       " 'jay',\n",
       " ',',\n",
       " 'that',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'a',\n",
       " 'fun',\n",
       " 'game',\n",
       " '\"',\n",
       " 'and',\n",
       " 'i',\n",
       " 'turned',\n",
       " 'around',\n",
       " 'right',\n",
       " 'before',\n",
       " 'it',\n",
       " 'turned',\n",
       " 'off',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'heard',\n",
       " 'the',\n",
       " 'audio',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " '99',\n",
       " '%',\n",
       " 'sure',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " '\"',\n",
       " 'kate',\n",
       " 'upton',\n",
       " 'game',\n",
       " 'of',\n",
       " 'war',\n",
       " '\"',\n",
       " 'commercial',\n",
       " 'cause',\n",
       " 'i',\n",
       " 'said',\n",
       " ':',\n",
       " '\"',\n",
       " 'oh',\n",
       " ',',\n",
       " 'i',\n",
       " 'missed',\n",
       " 'it',\n",
       " 'what',\n",
       " 'was',\n",
       " 'it',\n",
       " 'called',\n",
       " '?',\n",
       " '\"',\n",
       " 'and',\n",
       " 'he',\n",
       " 'said',\n",
       " '\"',\n",
       " 'game',\n",
       " 'of',\n",
       " 'war',\n",
       " 'or',\n",
       " 'something',\n",
       " '\"',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'he',\n",
       " 'was',\n",
       " 'hint',\n",
       " '##ing',\n",
       " 'at',\n",
       " 'it',\n",
       " 'cause',\n",
       " 'he',\n",
       " 'knew',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'same',\n",
       " 'girl',\n",
       " 'i',\n",
       " 'had',\n",
       " 'on',\n",
       " 'poster',\n",
       " 'in',\n",
       " 'my',\n",
       " 'room',\n",
       " '?',\n",
       " 'help',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58fa3feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2026, 12954,  2003,  3772,  2428,  6881,  1012,  2023,  2003,\n",
       "          6881,  2021,  1045,  4999,  2065,  1045,  2071,  2031,  2070,  2393,\n",
       "          1012,  2026, 12954,  2038,  2042,  3772,  4326,  2005,  1037,  2096,\n",
       "          2085,  1012,  2016,  2074,  2987,  1005,  1056,  2868,  4902,  1998,\n",
       "          2038,  1037,  8744, 13268,  3670,  1012,  2016,  2515,  9015,  2007,\n",
       "         16021,  5358,  6200,  1998,  2016,  2038,  2042,  2200,  1998,  1045,\n",
       "          2812,  2200, 13233,  1012,  2057,  2079,  2031,  2000,  2693,  2160,\n",
       "          2574,  1998,  2057,  2024, 11158,  2769,  2021,  2014,  9164,  2003,\n",
       "          2145,  2428, 14099,  2005,  2014,  1012,  2016,  2987,  1005,  1056,\n",
       "          4952,  2000,  2477,  1045,  2360,  1998,  2016,  2038,  2042, 13325,\n",
       "          2026, 10740,  1998,  2036,  5870,  2043,  2009,  1005,  1055,  2025,\n",
       "          6057,  1998,  2025,  5870,  2043,  2009,  2003,  6057,  1012,  2009,\n",
       "          1005,  1055,  2428, 15366,  2033,  1010,  2004,  1045,  1005,  1049,\n",
       "          2069,  2403,  1012,  2021,  2054,  2079,  1045,  2079,  1029,  2009,\n",
       "          1005,  1055,  2061,  9202,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  9596,  8146,  1059,  1013,  2026,  3611,  1029,  7632,  3071,\n",
       "           999,  1045,  1005,  1049,  6108,  2410,  2086,  2214,  1010,  1045,\n",
       "          2031,  1037,  3291,  2007,  2026,  3611,  1012,  1045,  2467,  2514,\n",
       "          9596,  2043,  2057,  1005,  2222,  3422,  2694,  1010,  2030,  2156,\n",
       "          7298,  1010,  4385,  1012,  2005,  2742,  2065,  1000,  5613,  1059,\n",
       "          1013,  1996,  3340,  1000,  2003,  2006,  1996,  2450,  2097,  2031,\n",
       "          8301,  8737,  2100,  5929,  2006,  1998,  1010,  1045,  2514,  2026,\n",
       "          2227,  2893,  2980,  1010,  1998,  1045,  2131, 10339,  1012,  2030,\n",
       "          2054,  1045,  2031,  1037,  7046,  3291,  2007,  2003,  2043,  2057,\n",
       "          1005,  2128,  2012, 24547, 22345,  1998,  2183,  2083,  1996,  4638,\n",
       "          2041,  2240,  1010,  2045,  1005,  1055,  2788,  1000, 20446,  1000,\n",
       "          7298,  2045,  2029,  2428,  3084,  2033, 10339,  1013,  9596,  1012,\n",
       "          2036,  2002,  2196,  2758,  2505,  2000,  2033,  2055,  3057,  2030,\n",
       "          2505,  1010,  2066,  2017,  2113,  2129,  3611,  1005,  1055,  2057,\n",
       "          1005,  2222, 18381,  2045,  4124,  2066,  3038,  1024,  1000,  2298,\n",
       "          2012,  2008,  3492,  3203,  1000,  2030,  2242,  2247,  2216,  3210,\n",
       "          1010,  2002,  2196,  2515,  2008,  1012,  1045,  2079,  2031,  1037,\n",
       "          5736, 26900, 13082,  1999,  2026,  2282,  1012,  2002,  1005,  1055,\n",
       "          2196,  2435,  2033,  1000,  1996,  2831,  1000,  2593,  1012,  2023,\n",
       "          3084,  2026,  8796,  2007,  4851,  2032,  2065,  1045,  2064,  2031,\n",
       "          1037,  6513,  1012,  2105,  2257,  2197,  2095,  2002,  1045,  2018,\n",
       "          2026, 12202,  9475,  2188,  4281,  1037,  5736, 26900,  3861,  1006,\n",
       "          2004,  2017,  2064,  2425,  1045,  2066,  2014,  1037,  2843,  1010,\n",
       "          8840,  2140,  1007,  1998,  2002,  2056,  1000,  2339,  2017,  2288,\n",
       "          3057,  1999, 17573, 11072,  2006,  2115, 12202,  2879,  1000,  2002,\n",
       "          2056,  2009,  1999,  1037, 16644,  2126,  1010,  1998,  3281,  1012,\n",
       "          2061,  2009,  2001,  2498,  4997,  1012,  1998,  1996,  2069,  2060,\n",
       "          2518,  2002,  2106,  2001,  1010,  2056,  2000,  2033,  2096,  1045,\n",
       "          2001,  2652, 12202,  2001,  1000,  6108,  1010,  2008,  3504,  2066,\n",
       "          1037,  4569,  2208,  1000,  1998,  1045,  2357,  2105,  2157,  2077,\n",
       "          2009,  2357,  2125,  1010,  1998,  1045,  2657,  1996,  5746,  1010,\n",
       "          1998,  1045,  1005,  1049,  5585,  1003,  2469,  2009,  2001,  1996,\n",
       "          1000,  5736, 26900,  2208,  1997,  2162,  1000,  3293,  3426,  1045,\n",
       "          2056,  1024,  1000,  2821,  1010,  1045,  4771,  2009,  2054,  2001,\n",
       "          2009,  2170,  1029,  1000,  1998,  2002,  2056,  1000,  2208,  1997,\n",
       "          2162,  2030,  2242,  1000,  2079,  2017,  2228,  2002,  2001,  9374,\n",
       "          2075,  2012,  2009,  3426,  2002,  2354,  2009,  2001,  1996,  2168,\n",
       "          2611,  1045,  2018,  2006, 13082,  1999,  2026,  2282,  1029,  2393,\n",
       "           999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(example_batch, padding='longest', return_tensors='pt')\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d584e32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 382])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0ef3819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "296229ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8018221939244bd8945043b16291fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fa6e638dbc4d5597260b767064152f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e419f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 3.7899e-01, -1.4712e-02,  2.7151e-01,  ..., -3.3954e-02,\n",
       "           4.5202e-01, -8.3304e-02],\n",
       "         [ 2.1995e-01,  5.1819e-02, -2.1898e-01,  ...,  3.0398e-01,\n",
       "           7.8992e-01,  3.4174e-01],\n",
       "         [ 6.9810e-01,  2.8701e-04,  4.5882e-01,  ..., -3.7120e-01,\n",
       "           3.5956e-02,  1.5919e-01],\n",
       "         ...,\n",
       "         [ 1.4884e-01, -7.5779e-01,  9.9337e-01,  ...,  1.2291e-01,\n",
       "           1.3551e-01, -4.3027e-01],\n",
       "         [ 3.0356e-01,  2.1034e-01,  1.6035e-01,  ..., -3.2684e-02,\n",
       "           1.0484e-01,  2.6159e-01],\n",
       "         [ 4.1300e-01,  2.2131e-01,  2.3085e-01,  ...,  1.1983e-01,\n",
       "           9.8253e-02,  7.9160e-02]],\n",
       "\n",
       "        [[-3.5623e-02, -4.4165e-01,  4.0529e-01,  ..., -4.3983e-01,\n",
       "           6.4437e-01,  2.9590e-01],\n",
       "         [-1.4378e+00,  7.0333e-01,  3.4683e-01,  ..., -5.8992e-01,\n",
       "           3.5098e-01, -1.7803e-01],\n",
       "         [-1.3149e+00,  1.2391e+00,  6.2656e-02,  ..., -7.4146e-01,\n",
       "          -2.1272e-01, -4.1994e-01],\n",
       "         ...,\n",
       "         [-2.0497e-02,  2.1624e-01,  1.0922e+00,  ...,  3.8382e-01,\n",
       "           6.3635e-02,  1.6630e-01],\n",
       "         [-9.5871e-01, -4.8458e-01, -1.8815e-01,  ...,  6.7120e-01,\n",
       "          -1.9869e-01,  2.5800e-02],\n",
       "         [-2.6565e-01,  4.1109e-01,  8.9315e-01,  ...,  3.0462e-01,\n",
       "           2.1547e-01, -2.9182e-01]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.6645, -0.5700, -0.9623,  ..., -0.7723, -0.6980,  0.7910],\n",
       "        [-0.2537, -0.4532, -0.9776,  ..., -0.9552, -0.5274,  0.4381]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**encoded)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d44fb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for text in df['text'].to_numpy():\n",
    "    encoded = tokenizer(text)\n",
    "    if len(encoded['input_ids']) > 512:\n",
    "        print(idx)\n",
    "        break\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "790e8def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just found out my father isn\\'t my father (after 28 years) So, yesterday I was helping my mother with her email problem on her laptop when I ran across an email from one of her long-time friends who she recently hasn\\'t been seeing eye-to-eye with. I normally don\\'t bother reading people\\'s email and this is especially the case with my mother. However, when archiving all of her old emails I ran across an emailed that was titled with my name. \\n\\nI started to read and it was an angry email directed at mother. It started off talking about how cold and calculating she was. It then went on to say that she couldn\\'t believe how my mother could lie to her to her own flesh and blood. **What I read next completely paralyzed me. It said that she had been lying to me my entire life about my real father who died and how my real father was mentally ill. It also mentioned that I was mentally ill and how it probably ran in the family.** I was devastated. You mean, the guy i\\'ve been calling dad my entire life is not even related to me? Am I really mentally ill? I don\\'t think I am, but do mentally ill people know that they\\'re mentally ill? She might have said that out of anger. But, I am 28 fucking years old, and i\\'m just now finding out that \"dad\" is not my biological father? \\n\\nKeeping in mind that my moms friend might have said some things out of pure anger, I still had to question my mother because I know people normally don\\'t pull stuff like this out of their asses. I sit down with my mother after taking a few minutes outside to gain my composure. I tell her that I read her email and what the email said. I then asked her how much truth there was to this. She immediately shuts everything off in the house and begins to talk.\\n\\nShe basically tells me that when she and my \"dad\" first got married, she had an affair on him with some guy because she and my \"dad\" were going through serious issues. After being \"involved\" with this other guy, my dad wanted her back. But this time, she was carrying two. My mom continues on to say that she didn\\'t know who the real father was, it could be \"dad\" or it could be the guy who died. Either way my father accepted her back under the condition that if she wanted to say, the other guy could not be in the picture in any way. \\n\\nHow could my mother lie to me my entire life about this? I can understand when I was younger, but i\\'m damn near 30 years old now. And she never bothered to tell me. If I hadn\\'t run across the email, I would have NEVER known about this. The worst part of about this is that if this guy really was father, she denied me the right to know who my real father was and she also denied a guy a right to see his son and he died not ever being able to hold or talk to his son. \\n\\nI\\'ve already had trust issues, and now they\\'re severely worse. The one person I thought I could trust to be 100% honest with me has lied to me my entire life. This whole thing is just surreal to me. And I\\'m having a hard time coping with it all. What a fucked up way to start off this new year. \\n\\nHappy 2015.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ex = df['text'].to_numpy()[2]\n",
    "text_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "425fc6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer(text_ex)\n",
    "len(encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c38af9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'just',\n",
       " 'found',\n",
       " 'out',\n",
       " 'my',\n",
       " 'father',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'my',\n",
       " 'father',\n",
       " '(',\n",
       " 'after',\n",
       " '28',\n",
       " 'years',\n",
       " ')',\n",
       " 'so',\n",
       " ',',\n",
       " 'yesterday',\n",
       " 'i',\n",
       " 'was',\n",
       " 'helping',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'with',\n",
       " 'her',\n",
       " 'email',\n",
       " 'problem',\n",
       " 'on',\n",
       " 'her',\n",
       " 'laptop',\n",
       " 'when',\n",
       " 'i',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'an',\n",
       " 'email',\n",
       " 'from',\n",
       " 'one',\n",
       " 'of',\n",
       " 'her',\n",
       " 'long',\n",
       " '-',\n",
       " 'time',\n",
       " 'friends',\n",
       " 'who',\n",
       " 'she',\n",
       " 'recently',\n",
       " 'hasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'been',\n",
       " 'seeing',\n",
       " 'eye',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'eye',\n",
       " 'with',\n",
       " '.',\n",
       " 'i',\n",
       " 'normally',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'bother',\n",
       " 'reading',\n",
       " 'people',\n",
       " \"'\",\n",
       " 's',\n",
       " 'email',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'especially',\n",
       " 'the',\n",
       " 'case',\n",
       " 'with',\n",
       " 'my',\n",
       " 'mother',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'when',\n",
       " 'arch',\n",
       " '##iving',\n",
       " 'all',\n",
       " 'of',\n",
       " 'her',\n",
       " 'old',\n",
       " 'emails',\n",
       " 'i',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'an',\n",
       " 'email',\n",
       " '##ed',\n",
       " 'that',\n",
       " 'was',\n",
       " 'titled',\n",
       " 'with',\n",
       " 'my',\n",
       " 'name',\n",
       " '.',\n",
       " 'i',\n",
       " 'started',\n",
       " 'to',\n",
       " 'read',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'an',\n",
       " 'angry',\n",
       " 'email',\n",
       " 'directed',\n",
       " 'at',\n",
       " 'mother',\n",
       " '.',\n",
       " 'it',\n",
       " 'started',\n",
       " 'off',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'how',\n",
       " 'cold',\n",
       " 'and',\n",
       " 'calculating',\n",
       " 'she',\n",
       " 'was',\n",
       " '.',\n",
       " 'it',\n",
       " 'then',\n",
       " 'went',\n",
       " 'on',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'she',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'believe',\n",
       " 'how',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'could',\n",
       " 'lie',\n",
       " 'to',\n",
       " 'her',\n",
       " 'to',\n",
       " 'her',\n",
       " 'own',\n",
       " 'flesh',\n",
       " 'and',\n",
       " 'blood',\n",
       " '.',\n",
       " '*',\n",
       " '*',\n",
       " 'what',\n",
       " 'i',\n",
       " 'read',\n",
       " 'next',\n",
       " 'completely',\n",
       " 'paralyzed',\n",
       " 'me',\n",
       " '.',\n",
       " 'it',\n",
       " 'said',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'been',\n",
       " 'lying',\n",
       " 'to',\n",
       " 'me',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'about',\n",
       " 'my',\n",
       " 'real',\n",
       " 'father',\n",
       " 'who',\n",
       " 'died',\n",
       " 'and',\n",
       " 'how',\n",
       " 'my',\n",
       " 'real',\n",
       " 'father',\n",
       " 'was',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " 'and',\n",
       " 'how',\n",
       " 'it',\n",
       " 'probably',\n",
       " 'ran',\n",
       " 'in',\n",
       " 'the',\n",
       " 'family',\n",
       " '.',\n",
       " '*',\n",
       " '*',\n",
       " 'i',\n",
       " 'was',\n",
       " 'devastated',\n",
       " '.',\n",
       " 'you',\n",
       " 'mean',\n",
       " ',',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'calling',\n",
       " 'dad',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'is',\n",
       " 'not',\n",
       " 'even',\n",
       " 'related',\n",
       " 'to',\n",
       " 'me',\n",
       " '?',\n",
       " 'am',\n",
       " 'i',\n",
       " 'really',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '?',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'think',\n",
       " 'i',\n",
       " 'am',\n",
       " ',',\n",
       " 'but',\n",
       " 'do',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " 'people',\n",
       " 'know',\n",
       " 'that',\n",
       " 'they',\n",
       " \"'\",\n",
       " 're',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '?',\n",
       " 'she',\n",
       " 'might',\n",
       " 'have',\n",
       " 'said',\n",
       " 'that',\n",
       " 'out',\n",
       " 'of',\n",
       " 'anger',\n",
       " '.',\n",
       " 'but',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " '28',\n",
       " 'fucking',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'just',\n",
       " 'now',\n",
       " 'finding',\n",
       " 'out',\n",
       " 'that',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'is',\n",
       " 'not',\n",
       " 'my',\n",
       " 'biological',\n",
       " 'father',\n",
       " '?',\n",
       " 'keeping',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'my',\n",
       " 'mom',\n",
       " '##s',\n",
       " 'friend',\n",
       " 'might',\n",
       " 'have',\n",
       " 'said',\n",
       " 'some',\n",
       " 'things',\n",
       " 'out',\n",
       " 'of',\n",
       " 'pure',\n",
       " 'anger',\n",
       " ',',\n",
       " 'i',\n",
       " 'still',\n",
       " 'had',\n",
       " 'to',\n",
       " 'question',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'because',\n",
       " 'i',\n",
       " 'know',\n",
       " 'people',\n",
       " 'normally',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'pull',\n",
       " 'stuff',\n",
       " 'like',\n",
       " 'this',\n",
       " 'out',\n",
       " 'of',\n",
       " 'their',\n",
       " 'ass',\n",
       " '##es',\n",
       " '.',\n",
       " 'i',\n",
       " 'sit',\n",
       " 'down',\n",
       " 'with',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'after',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'few',\n",
       " 'minutes',\n",
       " 'outside',\n",
       " 'to',\n",
       " 'gain',\n",
       " 'my',\n",
       " 'composure',\n",
       " '.',\n",
       " 'i',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'that',\n",
       " 'i',\n",
       " 'read',\n",
       " 'her',\n",
       " 'email',\n",
       " 'and',\n",
       " 'what',\n",
       " 'the',\n",
       " 'email',\n",
       " 'said',\n",
       " '.',\n",
       " 'i',\n",
       " 'then',\n",
       " 'asked',\n",
       " 'her',\n",
       " 'how',\n",
       " 'much',\n",
       " 'truth',\n",
       " 'there',\n",
       " 'was',\n",
       " 'to',\n",
       " 'this',\n",
       " '.',\n",
       " 'she',\n",
       " 'immediately',\n",
       " 'shut',\n",
       " '##s',\n",
       " 'everything',\n",
       " 'off',\n",
       " 'in',\n",
       " 'the',\n",
       " 'house',\n",
       " 'and',\n",
       " 'begins',\n",
       " 'to',\n",
       " 'talk',\n",
       " '.',\n",
       " 'she',\n",
       " 'basically',\n",
       " 'tells',\n",
       " 'me',\n",
       " 'that',\n",
       " 'when',\n",
       " 'she',\n",
       " 'and',\n",
       " 'my',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'first',\n",
       " 'got',\n",
       " 'married',\n",
       " ',',\n",
       " 'she',\n",
       " 'had',\n",
       " 'an',\n",
       " 'affair',\n",
       " 'on',\n",
       " 'him',\n",
       " 'with',\n",
       " 'some',\n",
       " 'guy',\n",
       " 'because',\n",
       " 'she',\n",
       " 'and',\n",
       " 'my',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'were',\n",
       " 'going',\n",
       " 'through',\n",
       " 'serious',\n",
       " 'issues',\n",
       " '.',\n",
       " 'after',\n",
       " 'being',\n",
       " '\"',\n",
       " 'involved',\n",
       " '\"',\n",
       " 'with',\n",
       " 'this',\n",
       " 'other',\n",
       " 'guy',\n",
       " ',',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'wanted',\n",
       " 'her',\n",
       " 'back',\n",
       " '.',\n",
       " 'but',\n",
       " 'this',\n",
       " 'time',\n",
       " ',',\n",
       " 'she',\n",
       " 'was',\n",
       " 'carrying',\n",
       " 'two',\n",
       " '.',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'continues',\n",
       " 'on',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'she',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'who',\n",
       " 'the',\n",
       " 'real',\n",
       " 'father',\n",
       " 'was',\n",
       " ',',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'or',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'died',\n",
       " '.',\n",
       " 'either',\n",
       " 'way',\n",
       " 'my',\n",
       " 'father',\n",
       " 'accepted',\n",
       " 'her',\n",
       " 'back',\n",
       " 'under',\n",
       " 'the',\n",
       " 'condition',\n",
       " 'that',\n",
       " 'if',\n",
       " 'she',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'say',\n",
       " ',',\n",
       " 'the',\n",
       " 'other',\n",
       " 'guy',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'in',\n",
       " 'any',\n",
       " 'way',\n",
       " '.',\n",
       " 'how',\n",
       " 'could',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'lie',\n",
       " 'to',\n",
       " 'me',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'about',\n",
       " 'this',\n",
       " '?',\n",
       " 'i',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'younger',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'damn',\n",
       " 'near',\n",
       " '30',\n",
       " 'years',\n",
       " 'old',\n",
       " 'now',\n",
       " '.',\n",
       " 'and',\n",
       " 'she',\n",
       " 'never',\n",
       " 'bothered',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'me',\n",
       " '.',\n",
       " 'if',\n",
       " 'i',\n",
       " 'hadn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'run',\n",
       " 'across',\n",
       " 'the',\n",
       " 'email',\n",
       " ',',\n",
       " 'i',\n",
       " 'would',\n",
       " 'have',\n",
       " 'never',\n",
       " 'known',\n",
       " 'about',\n",
       " 'this',\n",
       " '.',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'part',\n",
       " 'of',\n",
       " 'about',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'really',\n",
       " 'was',\n",
       " 'father',\n",
       " ',',\n",
       " 'she',\n",
       " 'denied',\n",
       " 'me',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'know',\n",
       " 'who',\n",
       " 'my',\n",
       " 'real',\n",
       " 'father',\n",
       " 'was',\n",
       " 'and',\n",
       " 'she',\n",
       " 'also',\n",
       " 'denied',\n",
       " 'a',\n",
       " 'guy',\n",
       " 'a',\n",
       " 'right',\n",
       " 'to',\n",
       " 'see',\n",
       " 'his',\n",
       " 'son',\n",
       " 'and',\n",
       " 'he',\n",
       " 'died',\n",
       " 'not',\n",
       " 'ever',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'or',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'his',\n",
       " 'son',\n",
       " '.',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'already',\n",
       " 'had',\n",
       " 'trust',\n",
       " 'issues',\n",
       " ',',\n",
       " 'and',\n",
       " 'now',\n",
       " 'they',\n",
       " \"'\",\n",
       " 're',\n",
       " 'severely',\n",
       " 'worse',\n",
       " '.',\n",
       " 'the',\n",
       " 'one',\n",
       " 'person',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'i',\n",
       " 'could',\n",
       " 'trust',\n",
       " 'to',\n",
       " 'be',\n",
       " '100',\n",
       " '%',\n",
       " 'honest',\n",
       " 'with',\n",
       " 'me',\n",
       " 'has',\n",
       " 'lied',\n",
       " 'to',\n",
       " 'me',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " '.',\n",
       " 'this',\n",
       " 'whole',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'just',\n",
       " 'surreal',\n",
       " 'to',\n",
       " 'me',\n",
       " '.',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'having',\n",
       " 'a',\n",
       " 'hard',\n",
       " 'time',\n",
       " 'coping',\n",
       " 'with',\n",
       " 'it',\n",
       " 'all',\n",
       " '.',\n",
       " 'what',\n",
       " 'a',\n",
       " 'fucked',\n",
       " 'up',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'off',\n",
       " 'this',\n",
       " 'new',\n",
       " 'year',\n",
       " '.',\n",
       " 'happy',\n",
       " '2015',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdc51ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_encoded = tokenizer(text_ex, truncation=True)\n",
    "len(t_encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adcb38c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'just',\n",
       " 'found',\n",
       " 'out',\n",
       " 'my',\n",
       " 'father',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'my',\n",
       " 'father',\n",
       " '(',\n",
       " 'after',\n",
       " '28',\n",
       " 'years',\n",
       " ')',\n",
       " 'so',\n",
       " ',',\n",
       " 'yesterday',\n",
       " 'i',\n",
       " 'was',\n",
       " 'helping',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'with',\n",
       " 'her',\n",
       " 'email',\n",
       " 'problem',\n",
       " 'on',\n",
       " 'her',\n",
       " 'laptop',\n",
       " 'when',\n",
       " 'i',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'an',\n",
       " 'email',\n",
       " 'from',\n",
       " 'one',\n",
       " 'of',\n",
       " 'her',\n",
       " 'long',\n",
       " '-',\n",
       " 'time',\n",
       " 'friends',\n",
       " 'who',\n",
       " 'she',\n",
       " 'recently',\n",
       " 'hasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'been',\n",
       " 'seeing',\n",
       " 'eye',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'eye',\n",
       " 'with',\n",
       " '.',\n",
       " 'i',\n",
       " 'normally',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'bother',\n",
       " 'reading',\n",
       " 'people',\n",
       " \"'\",\n",
       " 's',\n",
       " 'email',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'especially',\n",
       " 'the',\n",
       " 'case',\n",
       " 'with',\n",
       " 'my',\n",
       " 'mother',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'when',\n",
       " 'arch',\n",
       " '##iving',\n",
       " 'all',\n",
       " 'of',\n",
       " 'her',\n",
       " 'old',\n",
       " 'emails',\n",
       " 'i',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'an',\n",
       " 'email',\n",
       " '##ed',\n",
       " 'that',\n",
       " 'was',\n",
       " 'titled',\n",
       " 'with',\n",
       " 'my',\n",
       " 'name',\n",
       " '.',\n",
       " 'i',\n",
       " 'started',\n",
       " 'to',\n",
       " 'read',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'an',\n",
       " 'angry',\n",
       " 'email',\n",
       " 'directed',\n",
       " 'at',\n",
       " 'mother',\n",
       " '.',\n",
       " 'it',\n",
       " 'started',\n",
       " 'off',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'how',\n",
       " 'cold',\n",
       " 'and',\n",
       " 'calculating',\n",
       " 'she',\n",
       " 'was',\n",
       " '.',\n",
       " 'it',\n",
       " 'then',\n",
       " 'went',\n",
       " 'on',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'she',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'believe',\n",
       " 'how',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'could',\n",
       " 'lie',\n",
       " 'to',\n",
       " 'her',\n",
       " 'to',\n",
       " 'her',\n",
       " 'own',\n",
       " 'flesh',\n",
       " 'and',\n",
       " 'blood',\n",
       " '.',\n",
       " '*',\n",
       " '*',\n",
       " 'what',\n",
       " 'i',\n",
       " 'read',\n",
       " 'next',\n",
       " 'completely',\n",
       " 'paralyzed',\n",
       " 'me',\n",
       " '.',\n",
       " 'it',\n",
       " 'said',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'been',\n",
       " 'lying',\n",
       " 'to',\n",
       " 'me',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'about',\n",
       " 'my',\n",
       " 'real',\n",
       " 'father',\n",
       " 'who',\n",
       " 'died',\n",
       " 'and',\n",
       " 'how',\n",
       " 'my',\n",
       " 'real',\n",
       " 'father',\n",
       " 'was',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'i',\n",
       " 'was',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " 'and',\n",
       " 'how',\n",
       " 'it',\n",
       " 'probably',\n",
       " 'ran',\n",
       " 'in',\n",
       " 'the',\n",
       " 'family',\n",
       " '.',\n",
       " '*',\n",
       " '*',\n",
       " 'i',\n",
       " 'was',\n",
       " 'devastated',\n",
       " '.',\n",
       " 'you',\n",
       " 'mean',\n",
       " ',',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'calling',\n",
       " 'dad',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'life',\n",
       " 'is',\n",
       " 'not',\n",
       " 'even',\n",
       " 'related',\n",
       " 'to',\n",
       " 'me',\n",
       " '?',\n",
       " 'am',\n",
       " 'i',\n",
       " 'really',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '?',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'think',\n",
       " 'i',\n",
       " 'am',\n",
       " ',',\n",
       " 'but',\n",
       " 'do',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " 'people',\n",
       " 'know',\n",
       " 'that',\n",
       " 'they',\n",
       " \"'\",\n",
       " 're',\n",
       " 'mentally',\n",
       " 'ill',\n",
       " '?',\n",
       " 'she',\n",
       " 'might',\n",
       " 'have',\n",
       " 'said',\n",
       " 'that',\n",
       " 'out',\n",
       " 'of',\n",
       " 'anger',\n",
       " '.',\n",
       " 'but',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " '28',\n",
       " 'fucking',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'just',\n",
       " 'now',\n",
       " 'finding',\n",
       " 'out',\n",
       " 'that',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'is',\n",
       " 'not',\n",
       " 'my',\n",
       " 'biological',\n",
       " 'father',\n",
       " '?',\n",
       " 'keeping',\n",
       " 'in',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'my',\n",
       " 'mom',\n",
       " '##s',\n",
       " 'friend',\n",
       " 'might',\n",
       " 'have',\n",
       " 'said',\n",
       " 'some',\n",
       " 'things',\n",
       " 'out',\n",
       " 'of',\n",
       " 'pure',\n",
       " 'anger',\n",
       " ',',\n",
       " 'i',\n",
       " 'still',\n",
       " 'had',\n",
       " 'to',\n",
       " 'question',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'because',\n",
       " 'i',\n",
       " 'know',\n",
       " 'people',\n",
       " 'normally',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'pull',\n",
       " 'stuff',\n",
       " 'like',\n",
       " 'this',\n",
       " 'out',\n",
       " 'of',\n",
       " 'their',\n",
       " 'ass',\n",
       " '##es',\n",
       " '.',\n",
       " 'i',\n",
       " 'sit',\n",
       " 'down',\n",
       " 'with',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'after',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'few',\n",
       " 'minutes',\n",
       " 'outside',\n",
       " 'to',\n",
       " 'gain',\n",
       " 'my',\n",
       " 'composure',\n",
       " '.',\n",
       " 'i',\n",
       " 'tell',\n",
       " 'her',\n",
       " 'that',\n",
       " 'i',\n",
       " 'read',\n",
       " 'her',\n",
       " 'email',\n",
       " 'and',\n",
       " 'what',\n",
       " 'the',\n",
       " 'email',\n",
       " 'said',\n",
       " '.',\n",
       " 'i',\n",
       " 'then',\n",
       " 'asked',\n",
       " 'her',\n",
       " 'how',\n",
       " 'much',\n",
       " 'truth',\n",
       " 'there',\n",
       " 'was',\n",
       " 'to',\n",
       " 'this',\n",
       " '.',\n",
       " 'she',\n",
       " 'immediately',\n",
       " 'shut',\n",
       " '##s',\n",
       " 'everything',\n",
       " 'off',\n",
       " 'in',\n",
       " 'the',\n",
       " 'house',\n",
       " 'and',\n",
       " 'begins',\n",
       " 'to',\n",
       " 'talk',\n",
       " '.',\n",
       " 'she',\n",
       " 'basically',\n",
       " 'tells',\n",
       " 'me',\n",
       " 'that',\n",
       " 'when',\n",
       " 'she',\n",
       " 'and',\n",
       " 'my',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'first',\n",
       " 'got',\n",
       " 'married',\n",
       " ',',\n",
       " 'she',\n",
       " 'had',\n",
       " 'an',\n",
       " 'affair',\n",
       " 'on',\n",
       " 'him',\n",
       " 'with',\n",
       " 'some',\n",
       " 'guy',\n",
       " 'because',\n",
       " 'she',\n",
       " 'and',\n",
       " 'my',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'were',\n",
       " 'going',\n",
       " 'through',\n",
       " 'serious',\n",
       " 'issues',\n",
       " '.',\n",
       " 'after',\n",
       " 'being',\n",
       " '\"',\n",
       " 'involved',\n",
       " '\"',\n",
       " 'with',\n",
       " 'this',\n",
       " 'other',\n",
       " 'guy',\n",
       " ',',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'wanted',\n",
       " 'her',\n",
       " 'back',\n",
       " '.',\n",
       " 'but',\n",
       " 'this',\n",
       " 'time',\n",
       " ',',\n",
       " 'she',\n",
       " 'was',\n",
       " 'carrying',\n",
       " 'two',\n",
       " '.',\n",
       " 'my',\n",
       " 'mom',\n",
       " 'continues',\n",
       " 'on',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'she',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'know',\n",
       " 'who',\n",
       " 'the',\n",
       " 'real',\n",
       " 'father',\n",
       " 'was',\n",
       " ',',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " '\"',\n",
       " 'dad',\n",
       " '\"',\n",
       " 'or',\n",
       " 'it',\n",
       " 'could',\n",
       " 'be',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'who',\n",
       " 'died',\n",
       " '.',\n",
       " 'either',\n",
       " 'way',\n",
       " 'my',\n",
       " 'father',\n",
       " 'accepted',\n",
       " 'her',\n",
       " 'back',\n",
       " 'under',\n",
       " 'the',\n",
       " 'condition',\n",
       " 'that',\n",
       " 'if',\n",
       " 'she',\n",
       " 'wanted',\n",
       " 'to',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(t_encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "322b2e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f6f49e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 145 at dim 1 (got 382)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-85cb28ca4153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2286\u001b[0m                 )\n\u001b[1;32m   2287\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2288\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2289\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m         )\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         batch_outputs = self._batch_prepare_for_model(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    713\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                     )\n\u001b[0;32m--> 715\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    716\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "full_encoded = tokenizer(list(df['text'][:10]), truncation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
