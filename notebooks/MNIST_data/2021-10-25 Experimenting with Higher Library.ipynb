{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e3af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e6c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97db6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c28517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "user_embed_input_dim = 10\n",
    "x_dim = 20\n",
    "y_dim = 1\n",
    "num_users = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990a9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "\n",
    "class UserEmbedNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.Linear(100, 50)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class PredictionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e677c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "user1 = F.one_hot(torch.tensor(0), num_classes=10).float()\n",
    "user2 = F.one_hot(torch.tensor(1), num_classes=10).float()\n",
    "users = torch.stack([user1, user2])\n",
    "user_list = [user1, user2]\n",
    "\n",
    "train_x_1 = torch.rand(20, 20)\n",
    "val_x_1 = torch.rand(10, 20)\n",
    "train_x_2 = torch.rand(20, 20)\n",
    "val_x_2 = torch.rand(10, 20)\n",
    "\n",
    "weights1 = torch.normal(mean=torch.zeros(20), std=torch.ones(20))[:, None]\n",
    "weights2 = weights1 * -1\n",
    "\n",
    "train_y_1 = torch.sigmoid(torch.matmul(train_x_1, weights1)) > .5\n",
    "val_y_1 = torch.sigmoid(torch.matmul(val_x_1, weights1)) > .5\n",
    "train_y_2 = torch.sigmoid(torch.matmul(train_x_2, weights2)) > .5\n",
    "val_y_2 = torch.sigmoid(torch.matmul(val_x_2, weights2)) > .5\n",
    "\n",
    "train_x = torch.cat([train_x_1, train_x_2])\n",
    "train_y = torch.cat([train_y_1, train_y_2])\n",
    "\n",
    "val_x_list = [val_x_1, val_x_2]\n",
    "val_y_list = [val_y_1, val_y_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46fba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ace7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init user embed net and optimizer\n",
    "user_embed_net = UserEmbedNet(user_embed_input_dim)\n",
    "user_embed_opt = optim.Adam(user_embed_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f944b",
   "metadata": {},
   "source": [
    "Notes on model training:\n",
    "* loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. i.e., x.grad += dloss/dx\n",
    "* optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs x += -lr * x.grad. \n",
    "* optimizer.zero-grad() clears x.grad for every parameter x in the optimizer. It's important to call this before loss.backward() - otherwise you'll accumuate gradients from multiple passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a970506",
   "metadata": {},
   "source": [
    "I'm getting error:\n",
    "\n",
    "```\n",
    "RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\n",
    "```\n",
    "\n",
    "Why does this happen?\n",
    "- To reduce memory usage, during the .backward() call, all the intermediary results are deleted when they are not needed anymore. \n",
    "\n",
    "I think for my problem, what is happening is that the intermediary gradient steps between the user embed_net_params and the weights are being deleted.\n",
    "\n",
    "So we need to retain part of the graph. The parts having to do with the pred_net should get deleted by Python's garbarge collection anyway because they go out of scope after we move onto the next user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "549d9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train user embed net\n",
    "user_embed_net.train()\n",
    "n_outer_iter = 3\n",
    "n_inner_iter = 5\n",
    "\n",
    "for outer_step in range(n_outer_iter):\n",
    "    user_embed_opt.zero_grad()\n",
    "    # (1) get user embeddings\n",
    "    user_embeds = user_embed_net(users)\n",
    "    # (2) get user weights\n",
    "    lhs = user_embeds.repeat_interleave(num_users, dim=0)\n",
    "    rhs = user_embeds.repeat(num_users, 1)\n",
    "    W = cos_sim(lhs, rhs).reshape(num_users, num_users)\n",
    "    W = 1 - W\n",
    "    W = torch.exp(-W)\n",
    "    # (3) train prediction models for each user\n",
    "    user_losses = torch.zeros(num_users)\n",
    "    for idx, user in enumerate(user_list):\n",
    "        # get weights for user and convert to sample weights\n",
    "        user_weight_vec = torch.cat([W[idx][0].repeat(20, 1), W[idx][1].repeat(20, 1)])\n",
    "        pred_net = PredictionNet(x_dim)\n",
    "        # single user train and eval\n",
    "        inner_opt = torch.optim.SGD(pred_net.parameters(), lr=1e-1)\n",
    "        with higher.innerloop_ctx(pred_net, inner_opt) as (fnet, diffopt):\n",
    "            # train model\n",
    "            for _ in range(n_inner_iter):\n",
    "                logits = fnet(train_x)\n",
    "                step_losses = F.binary_cross_entropy_with_logits(logits, train_y.float(), reduction='none')\n",
    "                step_loss = (user_weight_vec * step_losses).sum()\n",
    "                diffopt.step(step_loss)\n",
    "            # eval model on user specific val data\n",
    "            logits = fnet(val_x_list[idx])\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, val_y_list[idx].float())\n",
    "            # update after just one user first to debug\n",
    "            # TODO later: switch to updating after iterating through all users\n",
    "            loss.backward(retain_graph=True)\n",
    "            user_losses[idx] = loss.detach().item()\n",
    "    # maybe it's okay if just to the opt step after multiple loss.backwards???\n",
    "    user_embed_opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
