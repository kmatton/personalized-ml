{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfe94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c658f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "from datasets import Dataset, list_metrics, load_metric\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from trainers.my_trainer import MyTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9761eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "gpuids = [0, 1, 2, 3]\n",
    "\n",
    "if gpuids is None or len(gpuids) == 0:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    print(\"Using CPU\")\n",
    "else:\n",
    "    gpuid_str = str(gpuids[0])\n",
    "    for gpuid in gpuids[1:]:\n",
    "        gpuid_str += \",{}\".format(gpuid)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid_str\n",
    "    print(\"Using GPU:{}\".format(gpuid_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df8634c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b1e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"model_name_or_path\": \"bert-base-uncased\",\n",
    "    \"tokenizer_name\": \"bert-base-uncased\",\n",
    "    \"cache_dir\": \"/data/ddmg/personalizedmentalhealth/reddit_project/cached_models\",\n",
    "    \"use_fast_tokenizer\": True,\n",
    "    \"config_name_or_path\": \"bert-base-uncased\",\n",
    "}\n",
    "\n",
    "data_args = {\n",
    "    \"data_files\": [\"/data/ddmg/personalizedmentalhealth/reddit_project/data/4_all_data.csv\"],\n",
    "    \"preprocessing_num_workers\": None,  # number of processes to use for the preprocessing\n",
    "    \"mlm_probability\": 0.15,  # ratio of tokens to mask for MLM loss\n",
    "    \"max_eval_samples\": None,  # for debugging purposes, truncate # of evaluation samples to this value if set,\n",
    "    \"data_split\": \"val\",  # evaluate just on this split of data (or all data if None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bc79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for data_path in data_args[\"data_files\"]:\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    df_list.append(df)\n",
    "data_df = pd.concat(df_list)\n",
    "if data_args[\"data_split\"] is not None:\n",
    "    data_df = data_df[data_df[\"data_split\"] == data_args[\"data_split\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac9bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b667f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": model_args[\"use_fast_tokenizer\"],\n",
    "    \"cache_dir\": model_args[\"cache_dir\"]\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args[\"tokenizer_name\"], **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993efe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args[\"cache_dir\"]\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args[\"config_name_or_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055d8e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    model_args[\"model_name_or_path\"],\n",
    "    config=config,\n",
    "    cache_dir=model_args[\"cache_dir\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db7ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,  # do dynamic padding to longest sequence in batch later\n",
    "        truncation=True,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef71845f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e7ec6311cf4d9d8bae7823c14b3e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    num_proc=data_args[\"preprocessing_num_workers\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1eadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=data_args[\"mlm_probability\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b66baf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "036e526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75e18683",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_accuracy,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93c8ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6452c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ddmg/users/kmatton/.conda/envs/rlm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10 (default, May 19 2021, 18:05:58) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 7.22.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n",
      "In [1]: type(logits)\n",
      "Out[1]: torch.Tensor\n",
      "\n",
      "In [2]: logits.shape\n",
      "Out[2]: torch.Size([32, 512, 30522])\n",
      "\n",
      "In [3]: %exit_raise\n",
      "\n"
     ]
    },
    {
     "ename": "KillEmbedded",
     "evalue": "Embedded IPython raising error, as user requested.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKillEmbedded\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-af9cd0277ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_eval_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_eval_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_eval_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_samples\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_eval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2002\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ddmg/personalizedmentalhealth/reddit_project/reddit-personalized-lm/notebooks/../src/trainers/my_trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# take argmax of logits to reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/IPython/terminal/embed.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m     shell = InteractiveShellEmbed.instance(_init_location_id='%s:%s' % (\n\u001b[1;32m    387\u001b[0m         frame.f_code.co_filename, frame.f_lineno), **kwargs)\n\u001b[0;32m--> 388\u001b[0;31m     shell(header=header, stack_depth=2, compile_flags=compile_flags,\n\u001b[0m\u001b[1;32m    389\u001b[0m         _call_location_id='%s:%s' % (frame.f_code.co_filename, frame.f_lineno))\n\u001b[1;32m    390\u001b[0m     \u001b[0mInteractiveShellEmbed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rlm/lib/python3.8/site-packages/IPython/terminal/embed.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, header, local_ns, module, dummy, stack_depth, global_ns, compile_flags, **kw)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKillEmbedded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embedded IPython raising error, as user requested.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKillEmbedded\u001b[0m: Embedded IPython raising error, as user requested."
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "max_eval_samples = data_args[\"max_eval_samples\"] if data_args[\"max_eval_samples\"] is not None else len(dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(dataset))\n",
    "try:\n",
    "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
